{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using TensorFlow's `text.BertTokenizer`. Based on the [Subword Tokenizer Tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer#setup) from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")\n",
    "from tensorflow_text.python.ops.normalize_ops import case_fold_utf8\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Load the source dataset from XXX, remove sentences that are too long or contains characters outside Latin alphabet no. 1. Select the desired number of sentences from the processed dataset as training on the full dataset will take too long. Split into a training and validation set and save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39b0819-57a5-4b1e-9fb6-bcfcb775f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 s, sys: 2.19 s, total: 43.1 s\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\n",
    "    config.RAW_DATA_PATH,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"eng\", \"nor\"],\n",
    "    on_bad_lines=\"skip\",\n",
    ").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04a191a-ffb7-422b-b05f-0e0f780755bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression for non Latin-1 characters\n",
    "non_latin1_regex = re.compile(r\"[^\\x00-\\xFF]\")\n",
    "\n",
    "\n",
    "def filter_sentence(row):\n",
    "    for sentence in row:\n",
    "        if len(sentence) > config.SENTENCE_MAX_LEN:\n",
    "            return False\n",
    "        if bool(non_latin1_regex.search(sentence)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "mask = df.apply(filter_sentence, axis=1)\n",
    "df = df[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a3f0ec-a4ac-4524-aba9-5cb4fe9de886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.03% of 18,605,836 sentences filtered out.\n"
     ]
    }
   ],
   "source": [
    "n_samples = mask.sum()\n",
    "n_sentences = mask.shape[0]\n",
    "print(\n",
    "    f\"{(1 - n_samples/n_sentences) * 100:.2f}% of {n_sentences:,} sentences filtered out.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee06791-99d4-4d23-b9af-6af7bead0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(df)\n",
    "dataset = dataset.shuffle(config.BUFFER_SIZE).take(config.N_SENTENCES)\n",
    "n_samples = min(config.N_SENTENCES, n_samples)\n",
    "val_size = int(n_samples * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac4c759-1577-4098-8354-f67e618e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.save(config.VAL_DATA_PATH)\n",
    "train_dataset.save(config.TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659455d1-05ac-458b-a338-15e209d89f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Betrally Sportsbook website is excellent in functionality content.\n",
      "Norwegian: Betrally Sportsbook nettstedet er utmerket i funksjonalitet innhold.\n"
     ]
    }
   ],
   "source": [
    "for eng, nor in train_dataset.take(1):\n",
    "    print(f\"English: {eng.numpy().decode('utf-8')}\")\n",
    "    print(f\"Norwegian: {nor.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a92afe-33b3-455a-8f0c-d3bb12213d4a",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabularies, one for English and one for Norwegian, based on a lower case subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a44650-93ad-4aa7-87e6-20efdc7faa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sample = train_dataset.take(config.N_SAMPLES_TOKENIZER)\n",
    "sample_eng = vocab_sample.map(lambda x: case_fold_utf8(x[0]))\n",
    "sample_nor = vocab_sample.map(lambda x: case_fold_utf8(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5314c4dd-fe32-4d6f-87e6-ec25e99b45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c65650e-1354-4656-bba2-1412f0da3454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 57s, sys: 3.95 s, total: 4min 1s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_vocab_args = dict(\n",
    "    vocab_size=config.VOCAB_SIZE,\n",
    "    reserved_tokens=config.RESERVED_TOKENS,\n",
    "    bert_tokenizer_params=config.BERT_TOKENIZER_PARAMS,\n",
    ")\n",
    "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    sample_eng.batch(1000).prefetch(2), **bert_vocab_args\n",
    ")\n",
    "nor_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    sample_nor.batch(1000).prefetch(2), **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66ccf23-af83-4957-afd4-8b03311a9995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '$', '%', '&']\n",
      "['ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'ü', 'ý', 'þ']\n",
      "['format', 'meet', 'plan', 'problem', 'return', 'solutions', 'class', 'etc', 'future', 'grand']\n",
      "['##ö', '##ù', '##ú', '##ü', '##ý', '##þ', '##́', '##̈', '##μ', '##⁄']\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab[:10])\n",
    "print(eng_vocab[100:110])\n",
    "print(eng_vocab[1000:1010])\n",
    "print(eng_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d81895e7-2f63-4b4a-8682-8810852a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_path = config.DATA_DIR + \"/eng_vocab.txt\"\n",
    "nor_vocab_path = config.DATA_DIR + \"/nor_vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b821c6a9-a8d2-4454-9af6-c300cce2a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(eng_vocab_path, eng_vocab)\n",
    "write_vocab_file(nor_vocab_path, nor_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a06bb-7ae9-4fc9-ae7f-1a69829ecb68",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build and test the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "577394a8-7bfb-46b0-806d-d4e005b72f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = text.BertTokenizer(\n",
    "    eng_vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    ")\n",
    "nor_tokenizer = text.BertTokenizer(\n",
    "    nor_vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ee84bd-0c7b-401e-9cbe-d3ab179656c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can help you to find the perfect mercato san severino hotel room for almost any occaision.\n",
      "i feel kinda dizzy!\n",
      "well apparently it does, hence the need for the definition of the term kip.\n"
     ]
    }
   ],
   "source": [
    "for eng_examples in sample_eng.batch(3).take(1):\n",
    "    for ex in eng_examples:\n",
    "        print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80a79075-98bb-4093-82b2-da04c629e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139, 133, 203, 119, 116, 151, 114, 298, 416, 5706, 6285, 556, 60, 3475, 2445, 129, 176, 120, 550, 172, 3124, 17]\n",
      "[50, 914, 1257, 154, 1763, 499, 499, 186, 4]\n",
      "[206, 539, 2257, 4606, 136, 414, 15, 5073, 114, 240, 120, 114, 4186, 117, 114, 1447, 52, 3153, 17]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = eng_tokenizer.tokenize(eng_examples)\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_batch = token_batch.merge_dims(-2, -1)\n",
    "\n",
    "for ex in token_batch.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ee69e66-3794-4f1a-8456-ec283933b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can help you to find the perfect me ##rc ##ato san s ##ever ##ino hotel room for almost any occaision .\n",
      "i feel kind ##a di ##z ##z ##y !\n",
      "well app ##are ##ntly it does , hence the need for the definition of the term k ##ip .\n"
     ]
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = tf.gather(eng_vocab, token_batch)\n",
    "# Join with spaces.\n",
    "for ex in tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1):\n",
    "    print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8647befb-273b-44f9-978e-aa66c932445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can help you to find the perfect mercato san severino hotel room for almost any occaision .\n",
      "i feel kinda dizzy !\n",
      "well apparently it does , hence the need for the definition of the term kip .\n"
     ]
    }
   ],
   "source": [
    "words = eng_tokenizer.detokenize(token_batch)\n",
    "for ex in tf.strings.reduce_join(words, separator=\" \", axis=-1):\n",
    "    print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3911-6484-4f09-aff2-1e83b3e62945",
   "metadata": {},
   "source": [
    "## Customization and export\n",
    "Define a custom tokenizer class that can be exported and used in the Transformer, including functionality for adding [START]/[END] tokens and cleaning up output after detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e0479a6-d7db-46a9-9b00-82563dea80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(config.RESERVED_TOKENS) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(config.RESERVED_TOKENS) == \"[END]\")\n",
    "\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count, 1], START)\n",
    "    ends = tf.fill([count, 1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=\" \", axis=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, config, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(\n",
    "            vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    "        )\n",
    "        self._reserved_tokens = config.RESERVED_TOKENS\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        )\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        strings = case_fold_utf8(strings)\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0aa8fdd-d842-4c68-b7f7-62257f911b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'we', b'can', b'help', b'you', b'to', b'find', b'the',\n",
       "  b'perfect', b'mercato', b'san', b'severino', b'hotel', b'room', b'for',\n",
       "  b'almost', b'any', b'occaision', b'.', b'[END]']                       ,\n",
       " [b'[START]', b'i', b'feel', b'kinda', b'dizzy', b'!', b'[END]'],\n",
       " [b'[START]', b'well', b'apparently', b'it', b'does', b',', b'hence',\n",
       "  b'the', b'need', b'for', b'the', b'definition', b'of', b'the', b'term',\n",
       "  b'kip', b'.', b'[END]']                                                ]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = eng_tokenizer.tokenize(eng_examples).merge_dims(-2, -1)\n",
    "token_batch = add_start_end(token_batch)\n",
    "words = eng_tokenizer.detokenize(token_batch)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52585fa3-b678-4345-958a-1d957d43b3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'we can help you to find the perfect mercato san severino hotel room for almost any occaision .',\n",
       "       b'i feel kinda dizzy !',\n",
       "       b'well apparently it does , hence the need for the definition of the term kip .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(config.RESERVED_TOKENS, words).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76820f71-7066-4930-9ec1-8024b8afb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.eng = CustomTokenizer(config, eng_vocab_path)\n",
    "tokenizers.nor = CustomTokenizer(config, nor_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd41c2a-8c17-4be4-8813-077758e38776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(tokenizers, config.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8e3cdb1-b29e-4cdf-b159-51e52dcefd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7585"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(config.TOKENIZER_PATH)\n",
    "reloaded_tokenizers.eng.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c978e03d-d200-47d3-85de-e9de384b2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 4864,  181, 2617, 4993, 5446, 4977,    4,    3]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.eng.tokenize([\"Hello TensorFlow!\"])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fee9cf6e-2c5a-4b59-9580-e6148514c3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'hell', b'##o', b'ten', b'##so', b'##rf', b'##low', b'!',\n",
       "  b'[END]']]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.eng.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aab771f5-0b36-487d-a723-38551bf5b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello tensorflow !\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizers.eng.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde03d25-340e-45b6-bd12-920d98f3e2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
