{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using TensorFlow's `text.BertTokenizer`. Based on the [Subword Tokenizer Tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer#setup) from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")\n",
    "from tensorflow_text.python.ops.normalize_ops import case_fold_utf8\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Load the source dataset from XXX, remove sentences that are too long or contains characters outside Latin alphabet no. 1. Select the desired number of sentences from the processed dataset as training on the full dataset will take too long. Split into a training and validation set and save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b0819-57a5-4b1e-9fb6-bcfcb775f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\n",
    "    config.RAW_DATA_PATH,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"eng\", \"nor\"],\n",
    "    on_bad_lines=\"skip\",\n",
    ").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a191a-ffb7-422b-b05f-0e0f780755bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression for non Latin-1 characters\n",
    "non_latin1_regex = re.compile(r\"[^\\x00-\\xFF]\")\n",
    "\n",
    "\n",
    "def filter_sentence(row):\n",
    "    for sentence in row:\n",
    "        if len(sentence) > config.SENTENCE_MAX_LEN:\n",
    "            return False\n",
    "        if bool(non_latin1_regex.search(sentence)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "mask = df.apply(filter_sentence, axis=1)\n",
    "df = df[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3f0ec-a4ac-4524-aba9-5cb4fe9de886",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = mask.sum()\n",
    "n_sentences = mask.shape[0]\n",
    "print(\n",
    "    f\"{(1 - n_samples/n_sentences) * 100:.2f}% of {n_sentences:,} sentences filtered out.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee06791-99d4-4d23-b9af-6af7bead0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(df)\n",
    "dataset = dataset.shuffle(config.BUFFER_SIZE).take(config.N_SENTENCES)\n",
    "n_samples = min(config.N_SENTENCES, n_samples)\n",
    "val_size = int(n_samples * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4c759-1577-4098-8354-f67e618e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.save(config.VAL_DATA_PATH)\n",
    "train_dataset.save(config.TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659455d1-05ac-458b-a338-15e209d89f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng, nor in train_dataset.take(1):\n",
    "    print(f\"English: {eng.numpy().decode('utf-8')}\")\n",
    "    print(f\"Norwegian: {nor.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a92afe-33b3-455a-8f0c-d3bb12213d4a",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabularies, one for English and one for Norwegian, based on a lower case subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a44650-93ad-4aa7-87e6-20efdc7faa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_eng = vocab_sample.map(lambda x: case_fold_utf8(x[0]))\n",
    "sample_nor = vocab_sample.map(lambda x: case_fold_utf8(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314c4dd-fe32-4d6f-87e6-ec25e99b45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65650e-1354-4656-bba2-1412f0da3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bert_vocab_args = dict(\n",
    "    vocab_size=config.VOCAB_SIZE,\n",
    "    reserved_tokens=config.RESERVED_TOKENS,\n",
    "    bert_tokenizer_params=config.BERT_TOKENIZER_PARAMS,\n",
    ")\n",
    "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    sample_eng.batch(1000).prefetch(2), **bert_vocab_args\n",
    ")\n",
    "nor_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    sample_nor.batch(1000).prefetch(2), **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ccf23-af83-4957-afd4-8b03311a9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eng_vocab[:10])\n",
    "print(eng_vocab[100:110])\n",
    "print(eng_vocab[1000:1010])\n",
    "print(eng_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81895e7-2f63-4b4a-8682-8810852a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_path = config.DATA_DIR + \"/eng_vocab.txt\"\n",
    "nor_vocab_path = config.DATA_DIR + \"/nor_vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821c6a9-a8d2-4454-9af6-c300cce2a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(eng_vocab_path, eng_vocab)\n",
    "write_vocab_file(nor_vocab_path, nor_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a06bb-7ae9-4fc9-ae7f-1a69829ecb68",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build and test the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577394a8-7bfb-46b0-806d-d4e005b72f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = text.BertTokenizer(\n",
    "    eng_vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    ")\n",
    "nor_tokenizer = text.BertTokenizer(\n",
    "    nor_vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee84bd-0c7b-401e-9cbe-d3ab179656c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng_examples in sample_eng.batch(3).take(1):\n",
    "    for ex in eng_examples:\n",
    "        print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a79075-98bb-4093-82b2-da04c629e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = eng_tokenizer.tokenize(eng_examples)\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_batch = token_batch.merge_dims(-2, -1)\n",
    "\n",
    "for ex in token_batch.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee69e66-3794-4f1a-8456-ec283933b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = tf.gather(eng_vocab, token_batch)\n",
    "# Join with spaces.\n",
    "for ex in tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1):\n",
    "    print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647befb-273b-44f9-978e-aa66c932445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = eng_tokenizer.detokenize(token_batch)\n",
    "for ex in tf.strings.reduce_join(words, separator=\" \", axis=-1):\n",
    "    print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3911-6484-4f09-aff2-1e83b3e62945",
   "metadata": {},
   "source": [
    "## Customization and export\n",
    "Define a custom tokenizer class that can be exported and used in the Transformer, including functionality for adding [START]/[END] tokens and cleaning up output after detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0479a6-d7db-46a9-9b00-82563dea80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(config.RESERVED_TOKENS) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(config.RESERVED_TOKENS) == \"[END]\")\n",
    "\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count, 1], START)\n",
    "    ends = tf.fill([count, 1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=\" \", axis=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, config, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(\n",
    "            vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    "        )\n",
    "        self._reserved_tokens = config.RESERVED_TOKENS\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        )\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        strings = case_fold_utf8(strings)\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa8fdd-d842-4c68-b7f7-62257f911b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_batch = eng_tokenizer.tokenize(eng_examples).merge_dims(-2, -1)\n",
    "token_batch = add_start_end(token_batch)\n",
    "words = eng_tokenizer.detokenize(token_batch)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52585fa3-b678-4345-958a-1d957d43b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_text(config.RESERVED_TOKENS, words).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76820f71-7066-4930-9ec1-8024b8afb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.eng = CustomTokenizer(config, eng_vocab_path)\n",
    "tokenizers.nor = CustomTokenizer(config, nor_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd41c2a-8c17-4be4-8813-077758e38776",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(tokenizers, config.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3cdb1-b29e-4cdf-b159-51e52dcefd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(config.TOKENIZER_PATH)\n",
    "reloaded_tokenizers.eng.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978e03d-d200-47d3-85de-e9de384b2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = reloaded_tokenizers.eng.tokenize([\"Hello TensorFlow!\"])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9cf6e-2c5a-4b59-9580-e6148514c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = reloaded_tokenizers.eng.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab771f5-0b36-487d-a723-38551bf5b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_trip = reloaded_tokenizers.eng.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde03d25-340e-45b6-bd12-920d98f3e2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
