{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using TensorFlow's `text.BertTokenizer`. Based on the [Subword Tokenizer Tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer#setup) from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Load the source dataset from XXX, split into a training and validation set and save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee06791-99d4-4d23-b9af-6af7bead0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    config.RAW_DATA_PATH, sep=\"\\t\", names=[\"eng\", \"spa\"], usecols=[0, 1]\n",
    ")\n",
    "n_samples = df.shape[0]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(df)\n",
    "dataset = dataset.shuffle(config.BUFFER_SIZE)\n",
    "val_size = int(n_samples * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac4c759-1577-4098-8354-f67e618e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.save(config.VAL_DATA_PATH)\n",
    "train_dataset.save(config.TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659455d1-05ac-458b-a338-15e209d89f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I like your phone.\n",
      "Spanish: Me gusta tu teléfono.\n"
     ]
    }
   ],
   "source": [
    "for eng, spa in train_dataset.take(1):\n",
    "    print(f\"English: {eng.numpy().decode('utf-8')}\")\n",
    "    print(f\"Spanish: {spa.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a92afe-33b3-455a-8f0c-d3bb12213d4a",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabularies, one for English and one for Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a44650-93ad-4aa7-87e6-20efdc7faa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng = train_dataset.map(lambda x: x[0])\n",
    "train_spa = train_dataset.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5314c4dd-fe32-4d6f-87e6-ec25e99b45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c65650e-1354-4656-bba2-1412f0da3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_args = dict(\n",
    "    vocab_size=config.VOCAB_SIZE,\n",
    "    reserved_tokens=config.RESERVED_TOKENS,\n",
    "    bert_tokenizer_params=config.BERT_TOKENIZER_PARAMS,\n",
    "    learn_params={},\n",
    ")\n",
    "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_eng.batch(1000).prefetch(2), **bert_vocab_args\n",
    ")\n",
    "spa_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_spa.batch(1000).prefetch(2), **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66ccf23-af83-4957-afd4-8b03311a9995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", '(']\n",
      "['how', 'her', 'time', 'has', 'as', 'did', 'will', 'very', 'here', 'didn']\n",
      "['ability', 'drank', 'law', 'plays', 'airport', 'begin', 'difference', 'learning', 'past', 'pick']\n",
      "['##:', '##;', '##?', '##j', '##q', '##v', '##z', '##°', '##’', '##€']\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab[:10])\n",
    "print(eng_vocab[100:110])\n",
    "print(eng_vocab[1000:1010])\n",
    "print(eng_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81895e7-2f63-4b4a-8682-8810852a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_path = config.DATA_DIR + \"/eng_vocab.txt\"\n",
    "spa_vocab_path = config.DATA_DIR + \"/spa_vocab.txt\"\n",
    "write_vocab_file(eng_vocab_path, eng_vocab)\n",
    "write_vocab_file(spa_vocab_path, spa_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a06bb-7ae9-4fc9-ae7f-1a69829ecb68",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build and test the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "577394a8-7bfb-46b0-806d-d4e005b72f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = text.BertTokenizer(\n",
    "    eng_vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    ")\n",
    "spa_tokenizer = text.BertTokenizer(\n",
    "    spa_vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ee84bd-0c7b-401e-9cbe-d3ab179656c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That guy has a screw loose!\n",
      "What's on TV?\n",
      "He's a bit drunk.\n"
     ]
    }
   ],
   "source": [
    "for eng_examples in train_eng.batch(3).take(1):\n",
    "    for ex in eng_examples:\n",
    "        print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a79075-98bb-4093-82b2-da04c629e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 1034, 103, 29, 47, 2092, 1447, 40, 2318, 1017, 4]\n",
      "[77, 8, 47, 86, 460, 28]\n",
      "[64, 8, 47, 29, 524, 797, 14]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = eng_tokenizer.tokenize(eng_examples)\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_batch = token_batch.merge_dims(-2, -1)\n",
    "\n",
    "for ex in token_batch.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ee69e66-3794-4f1a-8456-ec283933b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that guy has a s ##c ##rew l ##oo ##se !\n",
      "what ' s on tv ?\n",
      "he ' s a bit drunk .\n"
     ]
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = tf.gather(eng_vocab, token_batch)\n",
    "# Join with spaces.\n",
    "for ex in tf.strings.reduce_join(txt_tokens, separator=\" \", axis=-1):\n",
    "    print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8647befb-273b-44f9-978e-aa66c932445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that guy has a screw loose !\n",
      "what ' s on tv ?\n",
      "he ' s a bit drunk .\n"
     ]
    }
   ],
   "source": [
    "words = eng_tokenizer.detokenize(token_batch)\n",
    "for ex in tf.strings.reduce_join(words, separator=\" \", axis=-1):\n",
    "    print(ex.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a3911-6484-4f09-aff2-1e83b3e62945",
   "metadata": {},
   "source": [
    "## Customization and export\n",
    "Define a custom tokenizer class that can be exported and used in the Transformer, including functionality for adding [START]/[END] tokens and cleaning up output after detokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e0479a6-d7db-46a9-9b00-82563dea80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(config.RESERVED_TOKENS) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(config.RESERVED_TOKENS) == \"[END]\")\n",
    "\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count, 1], START)\n",
    "    ends = tf.fill([count, 1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=\" \", axis=-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, config, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(\n",
    "            vocab_path, **config.BERT_TOKENIZER_PARAMS\n",
    "        )\n",
    "        self._reserved_tokens = config.RESERVED_TOKENS\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string)\n",
    "        )\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64)\n",
    "        )\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0aa8fdd-d842-4c68-b7f7-62257f911b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'that', b'guy', b'has', b'a', b'screw', b'loose', b'!',\n",
       "  b'[END]']                                                           ,\n",
       " [b'[START]', b'what', b\"'\", b's', b'on', b'tv', b'?', b'[END]'],\n",
       " [b'[START]', b'he', b\"'\", b's', b'a', b'bit', b'drunk', b'.', b'[END]']]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = eng_tokenizer.tokenize(eng_examples).merge_dims(-2, -1)\n",
    "token_batch = add_start_end(token_batch)\n",
    "words = eng_tokenizer.detokenize(token_batch)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52585fa3-b678-4345-958a-1d957d43b3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'that guy has a screw loose !', b\"what ' s on tv ?\",\n",
       "       b\"he ' s a bit drunk .\"], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(config.RESERVED_TOKENS, words).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76820f71-7066-4930-9ec1-8024b8afb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.eng = CustomTokenizer(config, eng_vocab_path)\n",
    "tokenizers.spa = CustomTokenizer(config, spa_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbd41c2a-8c17-4be4-8813-077758e38776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(tokenizers, config.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8e3cdb1-b29e-4cdf-b159-51e52dcefd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2909"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(config.TOKENIZER_PATH)\n",
    "reloaded_tokenizers.eng.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c978e03d-d200-47d3-85de-e9de384b2f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 2231,  427,   78,  510,  962, 2002,    4,    3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.eng.tokenize([\"Hello TensorFlow!\"])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fee9cf6e-2c5a-4b59-9580-e6148514c3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'hello', b'ten', b'##s', b'##or', b'##f', b'##low', b'!',\n",
       "  b'[END]']]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.eng.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aab771f5-0b36-487d-a723-38551bf5b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello tensorflow !\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizers.eng.detokenize(tokens)\n",
    "print(round_trip.numpy()[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2680b-a348-4378-bc23-550d5e1710c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
