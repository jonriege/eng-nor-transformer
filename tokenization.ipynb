{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80daad4-92bc-4ab2-8ba2-2538db4e37a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "This notebook builds two subword tokenizers using Tensorflow's `text.BertTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf807e35-a411-4093-9ce1-d578f425592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "650ea534-a105-427b-92b4-53684cc76038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb9182-2760-4d4f-b07e-e7fcae88f9f9",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Load the source dataset from XXX and split into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9ee06791-99d4-4d23-b9af-6af7bead0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.TRAIN_PATH, sep=\"\\t\", names=[\"eng\", \"spa\"], usecols=[0, 1])\n",
    "n_samples =  df.shape[0]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(df).reshape(())\n",
    "dataset = dataset.shuffle(config.BUFFER_SIZE)\n",
    "val_size = int(n_samples * config.VALIDATION_SHARE)\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "659455d1-05ac-458b-a338-15e209d89f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Keep track of everything that looks promising.\n",
      "Spanish: Lleve un registro de todo lo que parezca prometedor.\n"
     ]
    }
   ],
   "source": [
    "for eng, spa in train_dataset.take(1):\n",
    "    print(f\"English: {eng.numpy().decode('utf-8')}\")\n",
    "    print(f\"Spanish: {spa.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a92afe-33b3-455a-8f0c-d3bb12213d4a",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Generate the vocabularies, one for English and one for Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1a44650-93ad-4aa7-87e6-20efdc7faa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng = train_dataset.map(lambda x: x[0])\n",
    "train_spa = train_dataset.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5314c4dd-fe32-4d6f-87e6-ec25e99b45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c65650e-1354-4656-bba2-1412f0da3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_args = dict(\n",
    "    vocab_size = config.VOCAB_SIZE,\n",
    "    reserved_tokens=config.RESERVED_TOKENS,\n",
    "    bert_tokenizer_params=config.BERT_TOKENIZER_PARAMS,\n",
    "    learn_params={},\n",
    ")\n",
    "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_eng.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n",
    "spa_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_spa.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c66ccf23-af83-4957-afd4-8b03311a9995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", '(']\n",
      "['how', 'her', 'time', 'has', 'as', 'did', 'very', 'about', 'all', 'will']\n",
      "['eats', 'hiding', 'inside', 'offer', 'shopping', 'supposed', 'teach', 'boss', 'church', 'deal']\n",
      "['##:', '##;', '##?', '##j', '##q', '##v', '##z', '##°', '##’', '##€']\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab[:10])\n",
    "print(eng_vocab[100:110])\n",
    "print(eng_vocab[1000:1010])\n",
    "print(eng_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d81895e7-2f63-4b4a-8682-8810852a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_path = config.DATA_DIR + '/eng_vocab.txt'\n",
    "spa_vocab_path = config.DATA_DIR + '/spa_vocab.txt'\n",
    "write_vocab_file(eng_vocab_path, eng_vocab)\n",
    "write_vocab_file(spa_vocab_path, spa_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a06bb-7ae9-4fc9-ae7f-1a69829ecb68",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Build the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "577394a8-7bfb-46b0-806d-d4e005b72f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 16:18:44.917909: I tensorflow/core/kernels/lookup_util.cc:414] Table trying to initialize from file data/eng_vocab.txt is already initialized.\n",
      "2023-11-26 16:18:44.926815: I tensorflow/core/kernels/lookup_util.cc:414] Table trying to initialize from file data/spa_vocab.txt is already initialized.\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = text.BertTokenizer(eng_vocab_path, **config.BERT_TOKENIZER_PARAMS)\n",
    "spa_tokenizer = text.BertTokenizer(spa_vocab_path, **config.BERT_TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e2ee84bd-0c7b-401e-9cbe-d3ab179656c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'Nothing is missing.' b'No falta nada.']\n",
      " [b\"They're inseparable.\" b'Ellas son inseparables.']\n",
      " [b'Leave me.' b'D\\xc3\\xa9jame.']], shape=(3, 2), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset.batch(3).take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a79075-98bb-4093-82b2-da04c629e757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
