{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9daa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb18a7ef-a158-409d-b7f5-82dd7451bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "366b2d7c-824d-47b1-8fe5-aa710969ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_nonbreaking(corpus, non_breaking_prefixes):\n",
    "  corpus_cleaned = corpus\n",
    "  # Add the string $$$ before the non breaking prefixes\n",
    "  # To avoid remove dots from some words\n",
    "  for prefix in non_breaking_prefixes:\n",
    "    corpus_cleaned = corpus_cleaned.replace(prefix, prefix + '$$$')\n",
    "  # Remove dots not at the end of a sentence\n",
    "  corpus_cleaned = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_cleaned)\n",
    "  # Remove the $$$ mark\n",
    "  corpus_cleaned = re.sub(r\"\\.\\$\\$\\$\", '', corpus_cleaned)\n",
    "  # Rmove multiple white spaces\n",
    "  corpus_cleaned = re.sub(r\"  +\", \" \", corpus_cleaned)\n",
    "\n",
    "def preprocess_text_nonbreaking(corpus: str, non_breaking_prefixes: list[str]):\n",
    "    \"\"\"Preprocesses text by handling non-breaking prefixes and dots within sentences.\n",
    "\n",
    "    Args:\n",
    "        corpus: The input text to be preprocessed.\n",
    "        non_breaking_prefixes: A list of non-breaking prefixes.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text with non-breaking prefixes and dots handled.\n",
    "    \"\"\"\n",
    "    # Add \"$$$\" after non-breaking prefixes to avoid dot removal\n",
    "    for prefix in non_breaking_prefixes:\n",
    "        corpus = corpus.replace(prefix, f\"{prefix}$$$\")\n",
    "\n",
    "    # Replace dots not at the end of sentences with \".$$$\"\n",
    "    corpus = re.sub(r\"\\.(?=[0-9a-zA-Z])\", \".$$$\", corpus)\n",
    "\n",
    "    # Remove the \"$$$\" marks\n",
    "    corpus = corpus.replace(\"$$$\", \"\")\n",
    "\n",
    "    # Remove multiple consecutive white spaces\n",
    "    corpus = re.sub(r\"  +\", \" \", corpus)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c47d4132-a309-4938-993a-57ee21e09539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "TRAIN_PATH = DATA_DIR + \"/spa.txt\"\n",
    "NONBREAKING_SPA_PATH = DATA_DIR + \"/nonbreaking_prefix.es\"\n",
    "NONBREAKING_ENG_PATH = DATA_DIR + \"/nonbreaking_prefix.en\"\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH, sep=\"\\t\", names=[\"eng\", \"spa\"], usecols=[0, 1])\n",
    "\n",
    "def load_nonbreaking_prefixes(filepath, encoding=\"utf8\"):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = [l.strip() for l in file.readlines()]\n",
    "    prefixes = [l + \".\" for l in lines if l != \"\" and \"#\" not in l]\n",
    "    return prefixes\n",
    "nonbreaking_prefixes_spa = load_nonbreaking_prefixes(NONBREAKING_SPA_PATH)\n",
    "nonbreaking_prefixes_eng = load_nonbreaking_prefixes(NONBREAKING_ENG_PATH)\n",
    "\n",
    "df[\"eng\"].apply(lambda x : preprocess_text_nonbreaking(x, nonbreaking_prefixes_eng)).iloc[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "033b4b4f-580d-4e12-8881-cbca94541f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"eng\"].iloc[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4941a8d-823f-4f36-bc7c-e4eda7605669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_nonbreaking(corpus, non_breaking_prefixes):\n",
    "  corpus_cleaned = corpus\n",
    "  # Add the string $$$ before the non breaking prefixes\n",
    "  # To avoid remove dots from some words\n",
    "  for prefix in non_breaking_prefixes:\n",
    "    corpus_cleaned = corpus_cleaned.replace(prefix, prefix + '$$$')\n",
    "  # Remove dots not at the end of a sentence\n",
    "  corpus_cleaned = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_cleaned)\n",
    "  # Remove the $$$ mark\n",
    "  corpus_cleaned = re.sub(r\"\\.\\$\\$\\$\", '', corpus_cleaned)\n",
    "  # Rmove multiple white spaces\n",
    "  corpus_cleaned = re.sub(r\"  +\", \" \", corpus_cleaned)\n",
    "\n",
    "  return corpus_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daaf3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    # Calculate the dot product, QK_transpose\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    # Get the scale factor\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    # Apply the scale factor to the dot product\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    # Apply masking when it is requiered\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    # dot product with Values\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39aeba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        # Calculate the dimension of every head or projection\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "        # Set the weight matrices for Q, K and V\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        # Set the weight matrix for the output of the multi-head attention W0\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
    "        # Set the dimension of the projections\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.n_heads,\n",
    "                 self.d_head)\n",
    "        # Split the input vectors\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        # Set the Query, Key and Value matrices\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        # Split Q, K y V between the heads or projections\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        # Apply the scaled dot product\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        # Get the attention scores\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        # Concat the h heads or projections\n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        # Apply W0 to get the output of the multi-head attention\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9689fdd-98b5-4450-9fc0-e310301860d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles # (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # input shape batch_size, seq_length, d_model\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        # Calculate the angles given the input\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        # Calculate the positional encodings\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        # Expand the encodings with a new dimension\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        \n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837e10e2-aaba-4289-85d1-0fffcd8911a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, n_heads, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Hidden units of the feed forward component\n",
    "        self.FFN_units = FFN_units\n",
    "        # Set the number of projectios or heads\n",
    "        self.n_heads = n_heads\n",
    "        # Dropout rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        # Build the multihead layer\n",
    "        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        # Layer Normalization\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # Fully connected feed forward layer\n",
    "        self.ffn1_relu = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.ffn2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        # Layer normalization\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        # Forward pass of the multi-head attention\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        # Call to the residual connection and layer normalization\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        # Call to the FC layer\n",
    "        outputs = self.ffn1_relu(attention)\n",
    "        outputs = self.ffn2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        # Call to residual connection and the layer normalization\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bda7aae-665d-4b91-b981-3b6aabab8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        # The embedding layer\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        # Positional encoding layer\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        # Stack of n layers of multi-head attention and FC\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        n_heads,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        # Get the embedding vectors\n",
    "        outputs = self.embedding(inputs)\n",
    "        # Scale the embeddings by sqrt of d_model\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Positional encodding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        # Call the stacked layers\n",
    "        for i in range(self.n_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a4b2165-bf8b-4926-a076-ef0a96ed1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, n_heads, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention, causal attention\n",
    "        self.multi_head_causal_attention = MultiHeadAttention(self.n_heads)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention, encoder-decoder attention \n",
    "        self.multi_head_enc_dec_attention = MultiHeadAttention(self.n_heads)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.ffn1_relu = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.ffn2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        # Call the masked causal attention\n",
    "        attention = self.multi_head_causal_attention(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        # Residual connection and layer normalization\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        # Call the encoder-decoder attention\n",
    "        attention_2 = self.multi_head_enc_dec_attention(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        # Residual connection and layer normalization\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        # Call the Feed forward\n",
    "        outputs = self.ffn1_relu(attention_2)\n",
    "        outputs = self.ffn2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        # Residual connection and layer normalization\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c2e059-e0e4-4f8c-b52a-44a1cac5e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        # Embedding layer\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        # Positional encoding layer\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        # Stacked layers of multi-head attention and feed forward\n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        n_heads,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        # Get the embedding vectors\n",
    "        outputs = self.embedding(inputs)\n",
    "        # Scale by sqrt of d_model\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Positional encodding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        # Call the stacked layers\n",
    "        for i in range(self.n_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a44937-dae1-4aee-b812-83e834a94995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        # Build the encoder\n",
    "        self.encoder = Encoder(n_layers,\n",
    "                               FFN_units,\n",
    "                               n_heads,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        # Build the decoder\n",
    "        self.decoder = Decoder(n_layers,\n",
    "                               FFN_units,\n",
    "                               n_heads,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        # build the linear transformation and softmax function\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n",
    "        # Create the mask for padding\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        # Create the mask for the causal attention\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        # Create the padding mask for the encoder\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        # Create the mask for the causal attention\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        # Create the mask for the encoder-decoder attention\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        # Call the encoder\n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        # Call the decoder\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        # Call the Linear and Softmax functions\n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae3a4fae-32e8-4da8-8add-7ed71b0e2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56250151-892c-442e-8ba0-5284383469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae436ae1-325f-462f-8f40-b03a286509d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(dataset, transformer, n_epochs, print_every=50):\n",
    "  ''' Train the transformer model for n_epochs using the data generator dataset'''\n",
    "  losses = []\n",
    "  accuracies = []\n",
    "  # In every epoch\n",
    "  for epoch in range(n_epochs):\n",
    "    print(\"Inicio del epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    # Reset the losss and accuracy calculations\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    # Get a batch of inputs and targets\n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        # Set the decoder inputs\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        # Set the target outputs, right shifted\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Call the transformer and get the predicted output\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        # Update the weights and optimizer\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        # Save and store the metrics\n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % print_every == 0:\n",
    "            losses.append(train_loss.result())\n",
    "            accuracies.append(train_accuracy.result())\n",
    "            print(\"Epoch {} Lote {} Pérdida {:.4f} Precisión {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    # Checkpoint the model on every epoch        \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} in {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time for 1 epoch: {} secs\\n\".format(time.time() - start))\n",
    "\n",
    "  return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d33057-0ef1-4719-bde9-314601797581",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_words_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create the Transformer model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m transformer \u001b[38;5;241m=\u001b[39m Transformer(vocab_size_enc\u001b[38;5;241m=\u001b[39m\u001b[43mnum_words_inputs\u001b[49m,\n\u001b[1;32m      5\u001b[0m                           vocab_size_dec\u001b[38;5;241m=\u001b[39mnum_words_output,\n\u001b[1;32m      6\u001b[0m                           d_model\u001b[38;5;241m=\u001b[39mD_MODEL,\n\u001b[1;32m      7\u001b[0m                           n_layers\u001b[38;5;241m=\u001b[39mN_LAYERS,\n\u001b[1;32m      8\u001b[0m                           FFN_units\u001b[38;5;241m=\u001b[39mFFN_UNITS,\n\u001b[1;32m      9\u001b[0m                           n_heads\u001b[38;5;241m=\u001b[39mN_HEADS,\n\u001b[1;32m     10\u001b[0m                           dropout_rate\u001b[38;5;241m=\u001b[39mDROPOUT_RATE)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define a categorical cross entropy loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss_object \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                                                             reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_words_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clean the session\n",
    "tf.keras.backend.clear_session()\n",
    "# Create the Transformer model\n",
    "transformer = Transformer(vocab_size_enc=num_words_inputs,\n",
    "                          vocab_size_dec=num_words_output,\n",
    "                          d_model=D_MODEL,\n",
    "                          n_layers=N_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          n_heads=N_HEADS,\n",
    "                          dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "# Define a categorical cross entropy loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "# Define a metric to store the mean loss of every epoch\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "# Define a matric to save the accuracy in every epoch\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "# Create the scheduler for learning rate decay\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "# Create the Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "#Create the Checkpoint \n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Las checkpoint restored.\")\n",
    "\n",
    "# Train the model\n",
    "losses, accuracies = main_train(dataset, transformer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412d29c0-a5ba-42f7-9ad7-33a817b63597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp_sentence, tokenizer_in, tokenizer_out, target_max_len):\n",
    "    # Tokenize the input sequence using the tokenizer_in\n",
    "    inp_sentence = sos_token_input + tokenizer_in.encode(inp_sentence) + eos_token_input\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "\n",
    "    # Set the initial output sentence to sos\n",
    "    out_sentence = sos_token_output\n",
    "    # Reshape the output\n",
    "    output = tf.expand_dims(out_sentence, axis=0)\n",
    "\n",
    "    # For max target len tokens\n",
    "    for _ in range(target_max_len):\n",
    "        # Call the transformer and get the logits \n",
    "        predictions = transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        # Extract the logists of the next word\n",
    "        prediction = predictions[:, -1:, :]\n",
    "        # The highest probability is taken\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        # Check if it is the eos token\n",
    "        if predicted_id == eos_token_output:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        # Concat the predicted word to the output sequence\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbb58f-e61b-4723-9d1d-ced468b06594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a177e6f-7226-43b5-ba85-d02b2c3a9542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf334-9257-48e1-acca-195f174b4169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
