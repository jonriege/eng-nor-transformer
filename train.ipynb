{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551d2fde-6720-4721-9518-0a9342ab7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3c8cbed-61e2-47f1-875d-4297c70f652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb18a7ef-a158-409d-b7f5-82dd7451bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_non_breaking_prefixes, sentence_boundary_disambiguation\n",
    "from model import Transformer, CustomSchedule, main_train\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47d4132-a309-4938-993a-57ee21e09539",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.TRAIN_PATH, sep=\"\\t\", names=[\"eng\", \"spa\"], usecols=[0, 1])\n",
    "\n",
    "nonbreaking_prefixes_spa = load_non_breaking_prefixes(config.NONBREAKING_SPA_PATH)\n",
    "nonbreaking_prefixes_eng = load_non_breaking_prefixes(config.NONBREAKING_ENG_PATH)\n",
    "\n",
    "df[\"spa\"] = df[\"spa\"].apply(lambda x : sentence_boundary_disambiguation(x, nonbreaking_prefixes_spa))\n",
    "df[\"eng\"] = df[\"eng\"].apply(lambda x : sentence_boundary_disambiguation(x, nonbreaking_prefixes_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4982d030-f190-4a09-ac00-e4a27659905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(corpus, config, vocab=None):\n",
    "    int_vectorize_layer = layers.TextVectorization(\n",
    "        max_tokens=config.VOCAB_SIZE,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=config.MAX_TOKENS + 1,\n",
    "        vocabulary=vocab\n",
    "    )\n",
    "    if vocab is None:\n",
    "        int_vectorize_layer.adapt(corpus)\n",
    "    vocab = int_vectorize_layer.get_vocabulary()\n",
    "    return int_vectorize_layer(corpus), vocab\n",
    "\n",
    "eng, eng_vocab = vectorize_text(df[\"eng\"], config)\n",
    "spa, spa_vocab = vectorize_text(df[\"spa\"], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "886d7564-07ad-4441-a4e8-3cc31a7fce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(eng, spa):\n",
    "    eng_input = eng[:, :config.MAX_TOKENS]\n",
    "\n",
    "    spa = spa[:, :config.MAX_TOKENS+1]\n",
    "    spa_input = spa[:, :-1]\n",
    "    spa_labels = spa[:, 1:]\n",
    "\n",
    "    return (eng_input, spa_input), spa_labels\n",
    "\n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(config.BUFFER_SIZE)\n",
    "      .batch(config.BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f820708c-d630-4eff-81f3-96efa74b6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((eng, spa))\n",
    "dataset = make_batches(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f27e85aa-1287-4788-90be-e342024d2d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "for (eng, spa), spa_labels in dataset.take(1):\n",
    "  break\n",
    "\n",
    "print(eng.shape)\n",
    "print(spa.shape)\n",
    "print(spa_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "684341d9-584a-4417-b866-fe3c79c1156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PositionalEmbedding\n",
    "embed_eng = PositionalEmbedding(vocab_size=config.VOCAB_SIZE, d_model=config.D_MODEL)\n",
    "embed_spa = PositionalEmbedding(vocab_size=config.VOCAB_SIZE, d_model=config.D_MODEL)\n",
    "\n",
    "eng_emb = embed_eng(eng)\n",
    "spa_emb = embed_spa(spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae0f5d9-b223-4f97-a868-567574f441dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a2da267-1aae-4369-ad7a-51e865ddf9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Transformer model\n",
    "transformer = Transformer(vocab_size_enc=config.VOCAB_SIZE,\n",
    "                          vocab_size_dec=config.VOCAB_SIZE,\n",
    "                          d_model=config.D_MODEL,\n",
    "                          n_layers=config.N_LAYERS,\n",
    "                          FFN_units=config.FFN_DIM,\n",
    "                          n_heads=config.N_HEADS,\n",
    "                          dropout_rate=config.DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335e7f20-863d-4d16-ac16-43ca167f45c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las checkpoint restored.\n",
      "Inicio del epoch 1\n",
      "Epoch 1 Lote 0 Pérdida 1.0381 Precisión 0.0015\n",
      "Epoch 1 Lote 100 Pérdida 0.6676 Precisión 0.0199\n",
      "Epoch 1 Lote 200 Pérdida 0.6064 Precisión 0.0251\n",
      "Epoch 1 Lote 300 Pérdida 0.5621 Precisión 0.0292\n",
      "Epoch 1 Lote 400 Pérdida 0.5283 Precisión 0.0325\n",
      "Epoch 1 Lote 500 Pérdida 0.5025 Precisión 0.0353\n",
      "Epoch 1 Lote 600 Pérdida 0.4830 Precisión 0.0377\n",
      "Epoch 1 Lote 700 Pérdida 0.4652 Precisión 0.0395\n",
      "Epoch 1 Lote 800 Pérdida 0.4504 Precisión 0.0411\n",
      "Epoch 1 Lote 900 Pérdida 0.4379 Precisión 0.0426\n",
      "Epoch 1 Lote 1000 Pérdida 0.4261 Precisión 0.0439\n",
      "Epoch 1 Lote 1100 Pérdida 0.4168 Precisión 0.0450\n",
      "Epoch 1 Lote 1200 Pérdida 0.4080 Precisión 0.0460\n",
      "Epoch 1 Lote 1300 Pérdida 0.4001 Precisión 0.0470\n",
      "Epoch 1 Lote 1400 Pérdida 0.3932 Precisión 0.0478\n",
      "Epoch 1 Lote 1500 Pérdida 0.3869 Precisión 0.0487\n",
      "Epoch 1 Lote 1600 Pérdida 0.3811 Precisión 0.0494\n",
      "Epoch 1 Lote 1700 Pérdida 0.3758 Precisión 0.0501\n",
      "Epoch 1 Lote 1800 Pérdida 0.3709 Precisión 0.0508\n",
      "Epoch 1 Lote 1900 Pérdida 0.3665 Precisión 0.0514\n",
      "Epoch 1 Lote 2000 Pérdida 0.3622 Precisión 0.0520\n",
      "Epoch 1 Lote 2100 Pérdida 0.3581 Precisión 0.0525\n",
      "Epoch 1 Lote 2200 Pérdida 0.3540 Precisión 0.0530\n",
      "Saving checkpoint for epoch 1 in checkpoints/ckpt-2\n",
      "Time for 1 epoch: 2035.2664878368378 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "losses, accuracies = main_train(dataset, transformer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "412d29c0-a5ba-42f7-9ad7-33a817b63597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentence, vocab_input, vocab_output, transformer, config):\n",
    "    input = f\"{config.SOS_TOKEN} {input_sentence} {config.EOS_TOKEN}\"\n",
    "    input_encoded, _ = vectorize_text([input], config, vocab=vocab_input)\n",
    "\n",
    "    # Set the initial output sentence to sos\n",
    "    output = config.SOS_TOKEN\n",
    "    output_encoded, _ = vectorize_text([output], config, vocab=vocab_input)\n",
    "\n",
    "    # For max target len tokens\n",
    "    for _ in range(config.MAX_SEQUENCE_LENGTH):\n",
    "        # Call the transformer and get the logits \n",
    "        predictions = transformer(input_encoded, output_encoded, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        # Extract the logists of the next word\n",
    "        prediction = predictions[:, -1:, :]\n",
    "        # The highest probability is taken\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int64)\n",
    "        print(predicted_id)\n",
    "        # Check if it is the eos token\n",
    "        if predicted_id == 3:\n",
    "            return tf.squeeze(output_encoded, axis=0)\n",
    "        # Concat the predicted word to the output sequence\n",
    "        output = tf.concat([output_encoded, predicted_id], axis=-1)\n",
    "    return tf.squeeze(output_encoded, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cfbb58f-e61b-4723-9d1d-ced468b06594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[142]], shape=(1, 1), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
       "array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Who are you?.\", eng_vocab, spa_vocab, transformer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a177e6f-7226-43b5-ba85-d02b2c3a9542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>spa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sos Go. eos</td>\n",
       "      <td>sos Ve. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sos Go. eos</td>\n",
       "      <td>sos Vete. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sos Go. eos</td>\n",
       "      <td>sos Vaya. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sos Go. eos</td>\n",
       "      <td>sos Váyase. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sos Hi. eos</td>\n",
       "      <td>sos Hola. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140863</th>\n",
       "      <td>sos A carbon footprint is the amount of carbon...</td>\n",
       "      <td>sos Una huella de carbono es la cantidad de co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140864</th>\n",
       "      <td>sos Since there are usually multiple websites ...</td>\n",
       "      <td>sos Como suele haber varias páginas web sobre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140865</th>\n",
       "      <td>sos If you want to sound like a native speaker...</td>\n",
       "      <td>sos Si quieres sonar como un hablante nativo, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140866</th>\n",
       "      <td>sos It may be impossible to get a completely e...</td>\n",
       "      <td>sos Puede que sea imposible obtener un corpus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140867</th>\n",
       "      <td>sos One day, I woke up to find that God had pu...</td>\n",
       "      <td>sos Un día, me desperté y vi que Dios me había...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140868 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "0                                             sos Go. eos   \n",
       "1                                             sos Go. eos   \n",
       "2                                             sos Go. eos   \n",
       "3                                             sos Go. eos   \n",
       "4                                             sos Hi. eos   \n",
       "...                                                   ...   \n",
       "140863  sos A carbon footprint is the amount of carbon...   \n",
       "140864  sos Since there are usually multiple websites ...   \n",
       "140865  sos If you want to sound like a native speaker...   \n",
       "140866  sos It may be impossible to get a completely e...   \n",
       "140867  sos One day, I woke up to find that God had pu...   \n",
       "\n",
       "                                                      spa  \n",
       "0                                             sos Ve. eos  \n",
       "1                                           sos Vete. eos  \n",
       "2                                           sos Vaya. eos  \n",
       "3                                         sos Váyase. eos  \n",
       "4                                           sos Hola. eos  \n",
       "...                                                   ...  \n",
       "140863  sos Una huella de carbono es la cantidad de co...  \n",
       "140864  sos Como suele haber varias páginas web sobre ...  \n",
       "140865  sos Si quieres sonar como un hablante nativo, ...  \n",
       "140866  sos Puede que sea imposible obtener un corpus ...  \n",
       "140867  sos Un día, me desperté y vi que Dios me había...  \n",
       "\n",
       "[140868 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf334-9257-48e1-acca-195f174b4169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
