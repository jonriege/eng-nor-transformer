{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "551d2fde-6720-4721-9518-0a9342ab7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3c8cbed-61e2-47f1-875d-4297c70f652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb18a7ef-a158-409d-b7f5-82dd7451bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_non_breaking_prefixes, sentence_boundary_disambiguation\n",
    "from model import Transformer, CustomSchedule, main_train\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c47d4132-a309-4938-993a-57ee21e09539",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.TRAIN_PATH, sep=\"\\t\", names=[\"eng\", \"spa\"], usecols=[0, 1])\n",
    "\n",
    "nonbreaking_prefixes_spa = load_non_breaking_prefixes(config.NONBREAKING_SPA_PATH)\n",
    "nonbreaking_prefixes_eng = load_non_breaking_prefixes(config.NONBREAKING_ENG_PATH)\n",
    "\n",
    "df[\"spa\"] = df[\"spa\"].apply(lambda x : sentence_boundary_disambiguation(x, nonbreaking_prefixes_spa))\n",
    "df[\"eng\"] = df[\"eng\"].apply(lambda x : sentence_boundary_disambiguation(x, nonbreaking_prefixes_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4982d030-f190-4a09-ac00-e4a27659905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(corpus, config, vocab=None):\n",
    "    int_vectorize_layer = layers.TextVectorization(\n",
    "        max_tokens=config.VOCAB_SIZE,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=config.MAX_SEQUENCE_LENGTH,\n",
    "        vocabulary=vocab\n",
    "    )\n",
    "    if vocab is None:\n",
    "        int_vectorize_layer.adapt(corpus)\n",
    "    vocab = int_vectorize_layer.get_vocabulary()\n",
    "    return int_vectorize_layer(corpus), vocab\n",
    "\n",
    "input, eng_vocab = vectorize_text(df[\"eng\"], config)\n",
    "output, spa_vocab = vectorize_text(df[\"spa\"], config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f820708c-d630-4eff-81f3-96efa74b6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input, output))\n",
    "dataset = dataset.shuffle(buffer_size=df.shape[0], reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size=config.BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dae0f5d9-b223-4f97-a868-567574f441dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a2da267-1aae-4369-ad7a-51e865ddf9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Transformer model\n",
    "transformer = Transformer(vocab_size_enc=config.VOCAB_SIZE,\n",
    "                          vocab_size_dec=config.VOCAB_SIZE,\n",
    "                          d_model=config.D_MODEL,\n",
    "                          n_layers=config.N_LAYERS,\n",
    "                          FFN_units=config.FFN_DIM,\n",
    "                          n_heads=config.N_HEADS,\n",
    "                          dropout_rate=config.DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e7f20-863d-4d16-ac16-43ca167f45c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las checkpoint restored.\n",
      "Inicio del epoch 1\n",
      "Epoch 1 Lote 0 Pérdida 1.0262 Precisión 0.0015\n",
      "Epoch 1 Lote 100 Pérdida 0.6582 Precisión 0.0197\n",
      "Epoch 1 Lote 200 Pérdida 0.6010 Precisión 0.0250\n",
      "Epoch 1 Lote 300 Pérdida 0.5583 Precisión 0.0293\n",
      "Epoch 1 Lote 400 Pérdida 0.5239 Precisión 0.0326\n",
      "Epoch 1 Lote 500 Pérdida 0.4982 Precisión 0.0353\n",
      "Epoch 1 Lote 600 Pérdida 0.4772 Precisión 0.0376\n",
      "Epoch 1 Lote 700 Pérdida 0.4608 Precisión 0.0395\n",
      "Epoch 1 Lote 800 Pérdida 0.4468 Precisión 0.0411\n",
      "Epoch 1 Lote 900 Pérdida 0.4348 Precisión 0.0425\n",
      "Epoch 1 Lote 1000 Pérdida 0.4244 Precisión 0.0438\n",
      "Epoch 1 Lote 1100 Pérdida 0.4147 Precisión 0.0450\n",
      "Epoch 1 Lote 1200 Pérdida 0.4064 Precisión 0.0460\n",
      "Epoch 1 Lote 1300 Pérdida 0.3988 Precisión 0.0469\n",
      "Epoch 1 Lote 1400 Pérdida 0.3919 Precisión 0.0478\n",
      "Epoch 1 Lote 1500 Pérdida 0.3857 Precisión 0.0486\n",
      "Epoch 1 Lote 1600 Pérdida 0.3800 Precisión 0.0494\n",
      "Epoch 1 Lote 1700 Pérdida 0.3745 Precisión 0.0500\n",
      "Epoch 1 Lote 1800 Pérdida 0.3696 Precisión 0.0507\n",
      "Epoch 1 Lote 1900 Pérdida 0.3650 Precisión 0.0513\n",
      "Epoch 1 Lote 2000 Pérdida 0.3606 Precisión 0.0519\n",
      "Epoch 1 Lote 2100 Pérdida 0.3567 Precisión 0.0524\n",
      "Epoch 1 Lote 2200 Pérdida 0.3528 Precisión 0.0529\n",
      "Inicio del epoch 2\n",
      "Epoch 2 Lote 0 Pérdida 0.2561 Precisión 0.0707\n",
      "Epoch 2 Lote 100 Pérdida 0.2564 Precisión 0.0655\n",
      "Epoch 2 Lote 200 Pérdida 0.2568 Precisión 0.0658\n",
      "Epoch 2 Lote 300 Pérdida 0.2566 Precisión 0.0656\n",
      "Epoch 2 Lote 400 Pérdida 0.2556 Precisión 0.0658\n",
      "Epoch 2 Lote 500 Pérdida 0.2550 Precisión 0.0659\n",
      "Epoch 2 Lote 600 Pérdida 0.2550 Precisión 0.0659\n",
      "Epoch 2 Lote 700 Pérdida 0.2543 Precisión 0.0661\n",
      "Epoch 2 Lote 800 Pérdida 0.2538 Precisión 0.0662\n",
      "Epoch 2 Lote 900 Pérdida 0.2535 Precisión 0.0664\n",
      "Epoch 2 Lote 1000 Pérdida 0.2520 Precisión 0.0665\n",
      "Epoch 2 Lote 1100 Pérdida 0.2515 Precisión 0.0666\n",
      "Epoch 2 Lote 1200 Pérdida 0.2506 Precisión 0.0667\n",
      "Epoch 2 Lote 1300 Pérdida 0.2496 Precisión 0.0668\n",
      "Epoch 2 Lote 1400 Pérdida 0.2491 Precisión 0.0669\n",
      "Epoch 2 Lote 1500 Pérdida 0.2485 Precisión 0.0670\n",
      "Epoch 2 Lote 1600 Pérdida 0.2479 Precisión 0.0671\n",
      "Epoch 2 Lote 1700 Pérdida 0.2474 Precisión 0.0671\n",
      "Epoch 2 Lote 1800 Pérdida 0.2466 Precisión 0.0673\n",
      "Epoch 2 Lote 1900 Pérdida 0.2459 Precisión 0.0674\n",
      "Epoch 2 Lote 2000 Pérdida 0.2453 Precisión 0.0674\n",
      "Epoch 2 Lote 2100 Pérdida 0.2446 Precisión 0.0675\n",
      "Epoch 2 Lote 2200 Pérdida 0.2440 Precisión 0.0676\n",
      "Inicio del epoch 3\n",
      "Epoch 3 Lote 0 Pérdida 0.2137 Precisión 0.0729\n",
      "Epoch 3 Lote 100 Pérdida 0.2138 Precisión 0.0709\n",
      "Epoch 3 Lote 200 Pérdida 0.2127 Precisión 0.0710\n",
      "Epoch 3 Lote 300 Pérdida 0.2154 Precisión 0.0711\n",
      "Epoch 3 Lote 400 Pérdida 0.2178 Precisión 0.0713\n",
      "Epoch 3 Lote 500 Pérdida 0.2173 Precisión 0.0715\n",
      "Epoch 3 Lote 600 Pérdida 0.2171 Precisión 0.0714\n",
      "Epoch 3 Lote 700 Pérdida 0.2171 Precisión 0.0714\n",
      "Epoch 3 Lote 800 Pérdida 0.2167 Precisión 0.0715\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "losses, accuracies = main_train(dataset, transformer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d29c0-a5ba-42f7-9ad7-33a817b63597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentence, vocab_input, vocab_output, transformer, config):\n",
    "    input = f\"{config.SOS_TOKEN} {input_sentence} {config.EOS_TOKEN}\"\n",
    "    input_encoded, _ = vectorize_text([input], config, vocab=vocab_input)\n",
    "\n",
    "    # Set the initial output sentence to sos\n",
    "    output = config.SOS_TOKEN\n",
    "    output_encoded, _ = vectorize_text([output], config, vocab=vocab_input)\n",
    "\n",
    "    # For max target len tokens\n",
    "    for _ in range(config.MAX_SEQUENCE_LENGTH):\n",
    "        # Call the transformer and get the logits \n",
    "        predictions = transformer(input_encoded, output_encoded, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        # Extract the logists of the next word\n",
    "        prediction = predictions[:, -1:, :]\n",
    "        # The highest probability is taken\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        print(predicted_id)\n",
    "        # Check if it is the eos token\n",
    "        if predicted_id == 2:\n",
    "            return tf.squeeze(output_encoded, axis=0)\n",
    "        # Concat the predicted word to the output sequence\n",
    "        output = tf.concat([output_encoded, predicted_id], axis=-1)\n",
    "    return tf.squeeze(output_encoded, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbb58f-e61b-4723-9d1d-ced468b06594",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Who are you?.\", eng_vocab, spa_vocab, transformer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a177e6f-7226-43b5-ba85-d02b2c3a9542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>spa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go. &lt;EOS&gt;</td>\n",
       "      <td>Ve. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go. &lt;EOS&gt;</td>\n",
       "      <td>Vete. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go. &lt;EOS&gt;</td>\n",
       "      <td>Vaya. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go. &lt;EOS&gt;</td>\n",
       "      <td>Váyase. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi. &lt;EOS&gt;</td>\n",
       "      <td>Hola. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140863</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Una huella de carbono es la cantidad de contam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140864</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Como suele haber varias páginas web sobre cual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140865</th>\n",
       "      <td>If you want to sound like a native speaker, yo...</td>\n",
       "      <td>Si quieres sonar como un hablante nativo, debe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140866</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Puede que sea imposible obtener un corpus comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140867</th>\n",
       "      <td>One day, I woke up to find that God had put ha...</td>\n",
       "      <td>Un día, me desperté y vi que Dios me había pue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140868 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "0                                               Go. <EOS>   \n",
       "1                                               Go. <EOS>   \n",
       "2                                               Go. <EOS>   \n",
       "3                                               Go. <EOS>   \n",
       "4                                               Hi. <EOS>   \n",
       "...                                                   ...   \n",
       "140863  A carbon footprint is the amount of carbon dio...   \n",
       "140864  Since there are usually multiple websites on a...   \n",
       "140865  If you want to sound like a native speaker, yo...   \n",
       "140866  It may be impossible to get a completely error...   \n",
       "140867  One day, I woke up to find that God had put ha...   \n",
       "\n",
       "                                                      spa  \n",
       "0                                               Ve. <EOS>  \n",
       "1                                             Vete. <EOS>  \n",
       "2                                             Vaya. <EOS>  \n",
       "3                                           Váyase. <EOS>  \n",
       "4                                             Hola. <EOS>  \n",
       "...                                                   ...  \n",
       "140863  Una huella de carbono es la cantidad de contam...  \n",
       "140864  Como suele haber varias páginas web sobre cual...  \n",
       "140865  Si quieres sonar como un hablante nativo, debe...  \n",
       "140866  Puede que sea imposible obtener un corpus comp...  \n",
       "140867  Un día, me desperté y vi que Dios me había pue...  \n",
       "\n",
       "[140868 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf334-9257-48e1-acca-195f174b4169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
