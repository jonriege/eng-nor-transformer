{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551d2fde-6720-4721-9518-0a9342ab7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c8cbed-61e2-47f1-875d-4297c70f652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb18a7ef-a158-409d-b7f5-82dd7451bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_non_breaking_prefixes, sentence_boundary_disambiguation\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47d4132-a309-4938-993a-57ee21e09539",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "TRAIN_PATH = DATA_DIR + \"/spa.txt\"\n",
    "NONBREAKING_SPA_PATH = DATA_DIR + \"/nonbreaking_prefix.es\"\n",
    "NONBREAKING_ENG_PATH = DATA_DIR + \"/nonbreaking_prefix.en\"\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH, sep=\"\\t\", names=[\"eng\", \"spa\"], usecols=[0, 1])\n",
    "\n",
    "nonbreaking_prefixes_spa = load_non_breaking_prefixes(NONBREAKING_SPA_PATH)\n",
    "nonbreaking_prefixes_eng = load_non_breaking_prefixes(NONBREAKING_ENG_PATH)\n",
    "\n",
    "df[\"spa\"] = df[\"spa\"].apply(lambda x : sentence_boundary_disambiguation(x, nonbreaking_prefixes_spa))\n",
    "df[\"eng\"] = df[\"eng\"].apply(lambda x : sentence_boundary_disambiguation(x, nonbreaking_prefixes_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f820708c-d630-4eff-81f3-96efa74b6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((df[\"spa\"], df[\"eng\"]))\n",
    "dataset = dataset.shuffle(buffer_size=df.shape[0], reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size=config.BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae0f5d9-b223-4f97-a868-567574f441dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affdc0c4-8d0e-4404-94e3-d1fdcb26d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce762ac-e71a-4c03-8a6c-63685d58e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_vectorize_layer.adapt(df[\"eng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da5f8572-6830-44cc-8568-068d6a5039a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'ran', 'for', 'mayor', 'eos', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "I ran for mayor. <EOS>\n"
     ]
    }
   ],
   "source": [
    "vectors = int_vectorize_layer(df[\"eng\"])\n",
    "vocab = int_vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daaf3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    # Calculate the dot product, QK_transpose\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    # Get the scale factor\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    # Apply the scale factor to the dot product\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    # Apply masking when it is requiered\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    # dot product with Values\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39aeba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        # Calculate the dimension of every head or projection\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "        # Set the weight matrices for Q, K and V\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        # Set the weight matrix for the output of the multi-head attention W0\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
    "        # Set the dimension of the projections\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.n_heads,\n",
    "                 self.d_head)\n",
    "        # Split the input vectors\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        # Set the Query, Key and Value matrices\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        # Split Q, K y V between the heads or projections\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        # Apply the scaled dot product\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        # Get the attention scores\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        # Concat the h heads or projections\n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        # Apply W0 to get the output of the multi-head attention\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9689fdd-98b5-4450-9fc0-e310301860d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles # (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # input shape batch_size, seq_length, d_model\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        # Calculate the angles given the input\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        # Calculate the positional encodings\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        # Expand the encodings with a new dimension\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        \n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837e10e2-aaba-4289-85d1-0fffcd8911a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, n_heads, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Hidden units of the feed forward component\n",
    "        self.FFN_units = FFN_units\n",
    "        # Set the number of projectios or heads\n",
    "        self.n_heads = n_heads\n",
    "        # Dropout rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        # Build the multihead layer\n",
    "        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        # Layer Normalization\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # Fully connected feed forward layer\n",
    "        self.ffn1_relu = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.ffn2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        # Layer normalization\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        # Forward pass of the multi-head attention\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        # Call to the residual connection and layer normalization\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        # Call to the FC layer\n",
    "        outputs = self.ffn1_relu(attention)\n",
    "        outputs = self.ffn2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        # Call to residual connection and the layer normalization\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bda7aae-665d-4b91-b981-3b6aabab8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        # The embedding layer\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        # Positional encoding layer\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        # Stack of n layers of multi-head attention and FC\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        n_heads,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        # Get the embedding vectors\n",
    "        outputs = self.embedding(inputs)\n",
    "        # Scale the embeddings by sqrt of d_model\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Positional encodding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        # Call the stacked layers\n",
    "        for i in range(self.n_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a4b2165-bf8b-4926-a076-ef0a96ed1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, n_heads, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention, causal attention\n",
    "        self.multi_head_causal_attention = MultiHeadAttention(self.n_heads)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention, encoder-decoder attention \n",
    "        self.multi_head_enc_dec_attention = MultiHeadAttention(self.n_heads)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.ffn1_relu = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.ffn2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        # Call the masked causal attention\n",
    "        attention = self.multi_head_causal_attention(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        # Residual connection and layer normalization\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        # Call the encoder-decoder attention\n",
    "        attention_2 = self.multi_head_enc_dec_attention(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        # Residual connection and layer normalization\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        # Call the Feed forward\n",
    "        outputs = self.ffn1_relu(attention_2)\n",
    "        outputs = self.ffn2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        # Residual connection and layer normalization\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c2e059-e0e4-4f8c-b52a-44a1cac5e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        # Embedding layer\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        # Positional encoding layer\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        # Stacked layers of multi-head attention and feed forward\n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        n_heads,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        # Get the embedding vectors\n",
    "        outputs = self.embedding(inputs)\n",
    "        # Scale by sqrt of d_model\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Positional encodding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        # Call the stacked layers\n",
    "        for i in range(self.n_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a44937-dae1-4aee-b812-83e834a94995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 n_layers,\n",
    "                 FFN_units,\n",
    "                 n_heads,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        # Build the encoder\n",
    "        self.encoder = Encoder(n_layers,\n",
    "                               FFN_units,\n",
    "                               n_heads,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        # Build the decoder\n",
    "        self.decoder = Decoder(n_layers,\n",
    "                               FFN_units,\n",
    "                               n_heads,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        # build the linear transformation and softmax function\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n",
    "        # Create the mask for padding\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        # Create the mask for the causal attention\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        # Create the padding mask for the encoder\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        # Create the mask for the causal attention\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        # Create the mask for the encoder-decoder attention\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        # Call the encoder\n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        # Call the decoder\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        # Call the Linear and Softmax functions\n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae3a4fae-32e8-4da8-8add-7ed71b0e2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56250151-892c-442e-8ba0-5284383469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae436ae1-325f-462f-8f40-b03a286509d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(dataset, transformer, n_epochs, print_every=50):\n",
    "  ''' Train the transformer model for n_epochs using the data generator dataset'''\n",
    "  losses = []\n",
    "  accuracies = []\n",
    "  # In every epoch\n",
    "  for epoch in range(n_epochs):\n",
    "    print(\"Inicio del epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    # Reset the losss and accuracy calculations\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    # Get a batch of inputs and targets\n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        # Set the decoder inputs\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        # Set the target outputs, right shifted\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Call the transformer and get the predicted output\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        # Update the weights and optimizer\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        # Save and store the metrics\n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % print_every == 0:\n",
    "            losses.append(train_loss.result())\n",
    "            accuracies.append(train_accuracy.result())\n",
    "            print(\"Epoch {} Lote {} Pérdida {:.4f} Precisión {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    # Checkpoint the model on every epoch        \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} in {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time for 1 epoch: {} secs\\n\".format(time.time() - start))\n",
    "\n",
    "  return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d33057-0ef1-4719-bde9-314601797581",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_words_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create the Transformer model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m transformer \u001b[38;5;241m=\u001b[39m Transformer(vocab_size_enc\u001b[38;5;241m=\u001b[39m\u001b[43mnum_words_inputs\u001b[49m,\n\u001b[1;32m      5\u001b[0m                           vocab_size_dec\u001b[38;5;241m=\u001b[39mnum_words_output,\n\u001b[1;32m      6\u001b[0m                           d_model\u001b[38;5;241m=\u001b[39mD_MODEL,\n\u001b[1;32m      7\u001b[0m                           n_layers\u001b[38;5;241m=\u001b[39mN_LAYERS,\n\u001b[1;32m      8\u001b[0m                           FFN_units\u001b[38;5;241m=\u001b[39mFFN_UNITS,\n\u001b[1;32m      9\u001b[0m                           n_heads\u001b[38;5;241m=\u001b[39mN_HEADS,\n\u001b[1;32m     10\u001b[0m                           dropout_rate\u001b[38;5;241m=\u001b[39mDROPOUT_RATE)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define a categorical cross entropy loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss_object \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                                                             reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_words_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the Transformer model\n",
    "transformer = Transformer(vocab_size_enc=num_words_inputs,\n",
    "                          vocab_size_dec=num_words_output,\n",
    "                          d_model=D_MODEL,\n",
    "                          n_layers=N_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          n_heads=N_HEADS,\n",
    "                          dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "# Define a categorical cross entropy loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "# Define a metric to store the mean loss of every epoch\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "# Define a matric to save the accuracy in every epoch\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "# Create the scheduler for learning rate decay\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "# Create the Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "#Create the Checkpoint \n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Las checkpoint restored.\")\n",
    "\n",
    "# Train the model\n",
    "losses, accuracies = main_train(dataset, transformer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412d29c0-a5ba-42f7-9ad7-33a817b63597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp_sentence, tokenizer_in, tokenizer_out, target_max_len):\n",
    "    # Tokenize the input sequence using the tokenizer_in\n",
    "    inp_sentence = sos_token_input + tokenizer_in.encode(inp_sentence) + eos_token_input\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "\n",
    "    # Set the initial output sentence to sos\n",
    "    out_sentence = sos_token_output\n",
    "    # Reshape the output\n",
    "    output = tf.expand_dims(out_sentence, axis=0)\n",
    "\n",
    "    # For max target len tokens\n",
    "    for _ in range(target_max_len):\n",
    "        # Call the transformer and get the logits \n",
    "        predictions = transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        # Extract the logists of the next word\n",
    "        prediction = predictions[:, -1:, :]\n",
    "        # The highest probability is taken\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        # Check if it is the eos token\n",
    "        if predicted_id == eos_token_output:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        # Concat the predicted word to the output sequence\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbb58f-e61b-4723-9d1d-ced468b06594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a177e6f-7226-43b5-ba85-d02b2c3a9542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcf334-9257-48e1-acca-195f174b4169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
